---
phase: 01-foundation-real-time-transcription
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - app/(app)/record/page.tsx
  - app/lib/hooks/use-audio-recorder.ts
  - app/lib/hooks/use-recording-timer.ts
  - app/lib/stores/recording-store.ts
  - app/components/audio/waveform-visualizer.tsx
  - app/components/audio/recording-controls.tsx
  - app/components/audio/live-transcript.tsx
  - convex/deepgram.ts
  - convex/recordings.ts
autonomous: true
user_setup:
  - service: deepgram
    why: "Speech-to-text transcription"
    env_vars:
      - name: DEEPGRAM_API_KEY
        source: "Deepgram Console (console.deepgram.com) -> Settings -> API Keys -> Create Key with 'Member' permissions"

must_haves:
  truths:
    - "User can start recording via microphone with a button press"
    - "User sees live waveform visualization during recording"
    - "User sees real-time transcription text appear as they speak"
    - "User sees speakers labeled (Speaker 1, Speaker 2) with timestamps"
    - "User can pause and resume recording"
    - "User sees elapsed recording time"
  artifacts:
    - path: "app/lib/hooks/use-audio-recorder.ts"
      provides: "MediaRecorder hook with format fallback and pause/resume"
      min_lines: 60
    - path: "convex/deepgram.ts"
      provides: "Server-side Deepgram WebSocket proxy action"
      contains: "DEEPGRAM_API_KEY"
    - path: "app/components/audio/live-transcript.tsx"
      provides: "Real-time transcript display with speaker labels and timestamps"
      min_lines: 40
    - path: "app/components/audio/recording-controls.tsx"
      provides: "Start/pause/resume/stop recording buttons with timer"
      min_lines: 30
    - path: "convex/recordings.ts"
      provides: "Audio file storage mutations"
      contains: "storage"
  key_links:
    - from: "app/lib/hooks/use-audio-recorder.ts"
      to: "convex/deepgram.ts"
      via: "Sends audio chunks to Convex action"
      pattern: "useAction.*deepgram"
    - from: "convex/deepgram.ts"
      to: "convex/transcripts.ts"
      via: "Stores transcription results"
      pattern: "appendWords|runMutation"
    - from: "app/components/audio/live-transcript.tsx"
      to: "convex/transcripts.ts"
      via: "Subscribes to words via useQuery"
      pattern: "useQuery.*getWords"
    - from: "app/(app)/record/page.tsx"
      to: "app/components/audio/*"
      via: "Composes recording UI"
      pattern: "RecordingControls|WaveformVisualizer|LiveTranscript"
---

<objective>
Build the complete recording interface with live audio capture, real-time Deepgram transcription via server-side proxy, waveform visualization, and recording controls (pause/resume/stop with timer).

Purpose: This is the core feature — the user records audio and sees real-time speaker-attributed transcription. Everything else in the app is built around this experience.
Output: Functional recording page where user presses record, sees waveform, hears themselves being transcribed in real-time with speaker labels, and can pause/resume with a visible timer.
</objective>

<execution_context>
@/Users/lucasalexander/.claude/get-shit-done/workflows/execute-plan.md
@/Users/lucasalexander/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-real-time-transcription/01-RESEARCH.md
@.planning/phases/01-foundation-real-time-transcription/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Audio recording hooks, Deepgram proxy, and audio storage</name>
  <files>
    app/lib/hooks/use-audio-recorder.ts
    app/lib/hooks/use-recording-timer.ts
    app/lib/stores/recording-store.ts
    convex/deepgram.ts
    convex/recordings.ts
  </files>
  <action>
    1. Create Zustand recording store (`app/lib/stores/recording-store.ts`):
       - State: status ("idle" | "recording" | "paused" | "stopped"), transcriptId (string | null), elapsedSeconds (number), error (string | null)
       - Actions: setStatus, setTranscriptId, incrementTimer, resetTimer, setError
       - Keep this minimal — it's shared state between components

    2. Create recording timer hook (`app/lib/hooks/use-recording-timer.ts`):
       - Uses setInterval (1 second) to increment elapsed time when status is "recording"
       - Pauses interval when status is "paused"
       - Resets on "stopped" or "idle"
       - Returns: elapsedSeconds, formatted time string (MM:SS)
       - Reads status from Zustand recording store

    3. Create audio recorder hook (`app/lib/hooks/use-audio-recorder.ts`):
       - On startRecording():
         a. Create transcript in Convex via useMutation (convex/transcripts.create)
         b. Call navigator.mediaDevices.getUserMedia with audio constraints:
            channelCount: 1, sampleRate: 16000, echoCancellation: true, noiseSuppression: true
         c. Detect supported MIME type using fallback chain:
            'audio/webm;codecs=opus' -> 'audio/webm' -> 'audio/ogg;codecs=opus' -> 'audio/mp4'
         d. Create MediaRecorder with detected MIME type, timeslice: 250ms (250ms chunks for low latency)
         e. On ondataavailable: validate blob size > 44 bytes (iOS bug check), convert to ArrayBuffer,
            call Convex action (deepgram.sendAudio) with transcriptId + audio bytes
         f. Collect all chunks in array for final recording blob
         g. Store recording store: status="recording", transcriptId
       - On pauseRecording(): mediaRecorder.pause(), update store status to "paused"
       - On resumeRecording(): mediaRecorder.resume(), update store status to "recording"
       - On stopRecording():
         a. mediaRecorder.stop()
         b. Stop all media stream tracks
         c. Call Convex action deepgram.closeConnection to finalize Deepgram session
         d. Combine all collected chunks into single Blob
         e. Upload final recording to Convex file storage via recordings.upload mutation
         f. Call transcripts.complete mutation with duration
         g. Update store: status="stopped"
         h. Return transcriptId for navigation to transcript view
       - On visibility change: if document.hidden and recording, auto-pause and show warning via store
       - Handle errors: catch getUserMedia NotAllowedError with helpful message, catch other errors
       - Cleanup: stop tracks and release stream on unmount

    4. Create Deepgram proxy action (`convex/deepgram.ts`):
       IMPORTANT: Convex actions run server-side, protecting the API key.

       Since Convex actions are stateless (no persistent WebSocket), use this architecture:
       - `sendAudio` action: accepts transcriptId (v.id("transcripts")) + audioChunk (v.bytes())
         a. Create Deepgram client with process.env.DEEPGRAM_API_KEY
         b. Use Deepgram's prerecorded API for each chunk (simpler than maintaining WebSocket state in stateless actions)
            OR use Deepgram REST API with the audio chunk
         c. Actually, for real-time streaming, the better pattern with Convex is:
            - Use a Next.js API route (`app/api/deepgram/route.ts`) as a WebSocket proxy instead
            - OR use Deepgram's REST pre-recorded endpoint per chunk (higher latency but works with stateless actions)

       REVISED APPROACH — Use Next.js API route for Deepgram WebSocket proxy:
       Create `app/api/deepgram/route.ts` instead:
       - This is a Next.js Route Handler that accepts POST with audio data
       - For real-time streaming, create a WebSocket endpoint or use Deepgram's callback URL feature

       SIMPLEST WORKING APPROACH for real-time:
       - Create `convex/deepgram.ts` action that accepts audio chunks and transcribes them using Deepgram's prerecorded (synchronous) API
       - Each 250ms-1s audio chunk gets sent to Deepgram prerecorded endpoint with diarize=true, punctuate=true, model=nova-2
       - Results come back synchronously, then stored via internal mutation to words table
       - This has slightly higher latency (500-1000ms per chunk) but works perfectly with Convex's stateless action model
       - For the user, it still feels "real-time" because chunks arrive every 250ms-1s

       Implementation:
       ```
       export const transcribeChunk = action({
         args: { transcriptId: v.id("transcripts"), audioData: v.bytes(), mimeType: v.string() },
         handler: async (ctx, args) => {
           // Use Deepgram REST API (prerecorded) for each audio chunk
           const response = await fetch("https://api.deepgram.com/v1/listen?model=nova-2&diarize=true&punctuate=true", {
             method: "POST",
             headers: {
               "Authorization": `Token ${process.env.DEEPGRAM_API_KEY}`,
               "Content-Type": args.mimeType,
             },
             body: Buffer.from(args.audioData),
           });
           const result = await response.json();

           // Extract words with speaker info and timestamps
           const words = result.results?.channels?.[0]?.alternatives?.[0]?.words || [];

           if (words.length > 0) {
             await ctx.runMutation(internal.transcripts.appendWords, {
               transcriptId: args.transcriptId,
               words: words.map(w => ({
                 text: w.word,
                 speaker: w.speaker,
                 startTime: w.start,
                 endTime: w.end,
                 isFinal: true,
               })),
             });
           }
         },
       });
       ```

       Note: We use Deepgram's REST prerecorded API (not WebSocket) because Convex actions are stateless — no persistent connections. Each audio chunk gets transcribed independently. Diarization quality may be slightly lower per-chunk vs. continuous stream, but this is the pragmatic approach. If needed, can upgrade to a Next.js WebSocket proxy later.

    5. Create recordings storage (`convex/recordings.ts`):
       - `generateUploadUrl` mutation: calls ctx.storage.generateUploadUrl() — used by client to get upload URL
       - `saveRecording` mutation: accepts transcriptId, storageId, format, size — inserts into recordings table
       - `getRecordingUrl` query: accepts transcriptId, returns storage URL via ctx.storage.getUrl(storageId)
       Ensure all mutations/queries check user authentication.
  </action>
  <verify>
    - `npx convex dev --once` deploys successfully (schema + functions)
    - `npm run build` succeeds
    - No TypeScript errors in hook files
    - Deepgram action is importable from convex/_generated/api
  </verify>
  <done>
    - Audio recorder hook handles start/pause/resume/stop with format fallback
    - Deepgram proxy action transcribes audio chunks server-side
    - Recording timer tracks elapsed time accurately
    - Audio chunks stored for final recording assembly
    - All Convex functions deployed and authenticated
  </done>
</task>

<task type="auto">
  <name>Task 2: Recording page UI with waveform, controls, and live transcript</name>
  <files>
    app/(app)/record/page.tsx
    app/components/audio/waveform-visualizer.tsx
    app/components/audio/recording-controls.tsx
    app/components/audio/live-transcript.tsx
  </files>
  <action>
    1. Create waveform visualizer (`app/components/audio/waveform-visualizer.tsx`):
       - Use react-voice-visualizer to show live waveform during recording
       - Import useVoiceVisualizer hook from react-voice-visualizer
       - Pass the MediaRecorder instance from useAudioRecorder
       - Style: full width, 80px height, burnt sienna (#D2691E) bars on cream background
       - Show flat line when paused, animated when recording
       - If react-voice-visualizer doesn't integrate cleanly, fall back to a simple canvas-based audio level meter using Web Audio API AnalyserNode
       - The component receives the MediaStream (not MediaRecorder) and creates its own AudioContext + AnalyserNode for visualization

    2. Create recording controls (`app/components/audio/recording-controls.tsx`):
       - Large circular "Record" button (center of screen, red with pulse animation when recording)
       - When idle: shows microphone icon, tap to start recording
       - When recording: shows pause icon, red pulsing border, tap to pause
       - When paused: shows play/resume icon, tap to resume
       - "Stop" button appears when recording or paused (smaller, below main button)
       - Timer display above buttons: MM:SS format, updates every second
       - Use Zustand recording store for state
       - All buttons use Tailwind: rounded-full, shadow-lg, transition-all
       - Mobile-first: buttons large enough for thumb tapping (min 56px touch target)

    3. Create live transcript display (`app/components/audio/live-transcript.tsx`):
       - Subscribe to words for current transcriptId via useQuery(api.transcripts.getWords, { transcriptId })
       - Group words by speaker: when speaker changes, start a new speaker block
       - Each speaker block shows:
         - Speaker label ("Speaker 1", "Speaker 2") in bold with a color dot (assign colors by speaker number)
         - Timestamp (formatted as MM:SS) at start of block
         - Text content (all words for that speaker segment joined with spaces)
       - Auto-scroll to bottom as new words arrive (use useEffect + scrollIntoView)
       - Empty state: "Start recording to see transcription..." in muted text
       - Batch rendering: use React.memo on speaker blocks to prevent unnecessary re-renders
       - Container: scrollable div with max height (calc(100vh - controls height)), padding for mobile

    4. Compose recording page (`app/(app)/record/page.tsx`):
       - Layout (top to bottom, mobile-first):
         a. Header: "Record" title, minimal
         b. Waveform visualizer (shows when recording/paused)
         c. Live transcript display (scrollable, takes remaining space)
         d. Recording controls (fixed at bottom of viewport)
       - Use useAudioRecorder hook for recording logic
       - Pass MediaStream to waveform visualizer
       - Pass transcriptId to live transcript display
       - On stop: navigate to /transcripts/[transcriptId] (transcript view page, built in Plan 03)
       - Handle errors: show toast/alert for permission denied, format not supported
       - Prevent navigation while recording (warn user)
       - Use flex column layout with controls pinned to bottom
  </action>
  <verify>
    - `npm run dev` and visit /record
    - Click Record button — browser requests microphone permission
    - Waveform visualization appears during recording
    - Timer counts up in MM:SS format
    - Words appear in live transcript as user speaks (with speaker labels)
    - Pause/resume works (timer pauses, waveform pauses, transcription pauses)
    - Stop recording stores audio and marks transcript complete
    - `npm run build` succeeds
  </verify>
  <done>
    - User can tap Record and see waveform + timer start
    - Real-time transcription text appears as user speaks
    - Speaker labels (Speaker 1, Speaker 2) shown with timestamps
    - Pause pauses everything, resume continues
    - Stop finalizes recording and transcript
    - All controls are thumb-friendly on mobile (56px+ touch targets)
  </done>
</task>

</tasks>

<verification>
1. Complete recording flow: tap Record -> speak -> see transcription appear in real-time -> pause -> resume -> stop
2. Speaker diarization works (test with 2 different voices or distinct audio)
3. Timer accurately tracks elapsed time including pauses
4. Waveform visualization responds to audio input
5. Audio recording blob is saved to Convex storage
6. Transcript words are stored in database with speaker and timestamp data
7. Mobile layout works (controls accessible, transcript scrollable)
</verification>

<success_criteria>
- REC-01: User records via microphone with live waveform visualization
- REC-03: User can pause and resume recording
- REC-04: User sees timer showing elapsed recording duration
- TRX-01: Real-time transcription appears as user speaks
- TRX-02: Speakers automatically detected and labeled (Speaker 1, Speaker 2)
- TRX-05: Timestamps shown alongside speaker text (in live transcript view)
- Audio chunks proxied through Convex action (API key never exposed to browser)
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-real-time-transcription/01-02-SUMMARY.md`
</output>
